---
title: 'Problem Set 1: Linear Methods'
author: "Lisa Büx (441574)
         Jan-Luka Scheewel (518556)
         Anastasia Suckau (440938)"
date: "2020-11-13 till 2020-11-29"
output:
  html_document:
    df_print: paged
  html_notebook: default
  pdf_document: default
subtitle: Machine Learning in R - WWU Muenster term 2020/2021
urlcolor: blue
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  #set comment formatting
  comment = ">",
  #collapse code and output
  collapse = F
)
```

```{r load-packages, include = FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load(tidyverse, GLMsData, ISLR, rsample, boot, MASS, class, glmnet, leaps, yardstick)
if (!require("gridExtra")) install.packages("gridExtra")
library("gridExtra")
library("ggplot2")
if (!require("caTools")) install.packages("caTools")
library("caTools")
if (!require("plotrix"))install.packages("plotrix")
library("plotrix")
if (!require("moderndive"))install.packages("moderndive")
library("moderndive")
if (!require("boot"))install.packages("boot")
library("boot")
if (!require("Rmisc"))install.packages("Rmisc")
library("Rmisc")
```


# Task 1: Multiple Linear Regression
## *Lung capacity data set*

*Load the data and convert the Smoke variable from int to fct.*
```{r}
data("lungcap", package = "GLMsData")
lungcap <- lungcap %>% 
  tibble::as_tibble() %>%                       
  dplyr::mutate(across(Smoke, as.factor))               
lungcap
```


*Transform the height from inches to cm and fit a multiple linear regression model to the data. The log of the Lung capacity as the response variable and age, height, gender and smoking habit as the predictors.*
```{r}
lungcap <- lungcap %>%
dplyr::mutate(across(Ht, ~ . * 2.54)) %>%
dplyr::rename(Htcm = Ht)
lung_model <- lungcap %>%
lm(log(FEV) ~ Age + Htcm + Gender + Smoke, data = .)     
summary(lung_model)
```


## Task 1.1 
*Write down the generic linear regression equation as well as the specific equation for the fitted `lung_model` including the point estimates for the coefficients.*

> The generic linear regression equation is given as: $$Y_i = a + bX_i$$ , where X is the explanatory variable and Y is the dependent variable. 

> The specific equation for the fitted `lung_model` is given by: $$log(FEV_i)= \beta_0 + \beta_1*Age_i + \beta_2*Htcm_i + \beta_3*Gender_i +\beta_4*Smoke_i + \epsilon_i$$ 

> Running the`lung_model` in R results in the following estimates: $$log(FEV_i)= -1.943998 + 0.023387*Age_i + 0.016849*Htcm_i + 0.029319*Gender_i - 0.046067*Smoke_i + errorterm_i$$

## Task 1.2
*Why is `log(FEV)` used as the response instead of `FEV`? To answer this question, plot `FEV` and `log(FEV)` using the `geom_density()` function as part of your `ggplot2` pipeline. What shape do you observe?*


```{r}
FEVplot <- ggplot(lungcap, aes(FEV)) + 
           geom_density(fill="yellow", alpha=0.1)+ 
           geom_vline(aes(xintercept=mean(FEV)), color="orange", linetype="solid", size=1) + 
           geom_vline(aes(xintercept=median(FEV)), color="purple", linetype="solid", size=1) +
           ggtitle("Density Plot (mean & median line) ") +
           xlab("FEV (Forced expiratory volume in liters)") + 
           ylab("Density")
logFEVplot <- ggplot(lungcap, aes(log(FEV))) + 
              geom_density(fill="yellow", alpha=0.1)+ 
              geom_vline(aes(xintercept=mean(log(FEV))), color="orange", linetype="solid", size=1) + 
              geom_vline(aes(xintercept=median(log(FEV))), color="purple", linetype="solid", size=1) +
              ggtitle("Density Plot (mean & median line)") +
              xlab("logFEV (Forced expiratory volume in liters)") + 
              ylab("Density")
grid.arrange(FEVplot, logFEVplot, ncol = 2, nrow = 1)
```

> A perfect normal (bell-shaped) distribution of the observed data set is not given. The data points follow approximately a log-normal distribution with a right skew so that its mean is bigger than its median as seen in the left plot.

> Having this in mind, a possible method is the log-normal transformation in order to obtain data fitting to a normal distribution as much as possible and therefore increase the validity of the statistical analysis. The reason for normal distributed data giving a more valid regression model, is based on the central limit theorem. It states that the mean of all samples from the same population will be approximately equal to the mean of the population. This enables us to make predictions about the true population from our sample data. We can see that the log-transformation reduced the right-skewness.

> Moreover, taking the scale into account, the difference between median and mean decreases substantially. With decreasing difference between these values, the symmetry around the mean of the distribution increases.

## Task 1.3

*Explain with your own words and numerical examples what the following statistics in the `summary(lung_model)` output mean.* 

```{r}
original_lung_model <- lungcap %>%
lm(FEV ~ Age + Htcm + Gender + Smoke, data = .)  
summary(lung_model)
summary(original_lung_model)
```

*i. `Estimate`*

> The estimate \((\hat{\beta})\) is the extimated regression coefficient and describes the relationship between a specific predictor variable and the response variable when all other predictor variables in the linear model are held constant and the specific predictor variavle is changed by one unit (For instance from \(x_j\) to \(x_{j+1}\)). 
> Formally the best OLS estimate can be expressed as \(\hat{\beta} = (X^T X)^{-1}X^T Y\). Where \(Y\) is an (\(n \times 1\)) vector of response variables (random sample),\(X\) is an \((n \times
2)\) matrix called the design matrix,

>In our model the response variable is the forced expiratory volume in liters, `FEV`, a measurement for lung capacity. The model estimates the effect of age, gender, height and smoking habits on the lung capacity.

>1. estimate for the variable `Age` (continous): 

>  + log of `FEV`: The coefficient estimate for the explanatory variable `Age` is equal to 0.023387 for the transformed data. That means, if `Age` is increased by one unit (year) the expiratory volume increases by approximately 2.3387% (= $100*\hat{\beta} \%$) on average.

>  + original scale of `FEV`: The coefficient estimate for the explanatory variable `Age` is equal to 0.065509 for the original scale of `FEV`. That means, if `Age` is increased by one unit (year) the expiratory volume increases by 0.065509 liter on average.

> 2. estimate for the variable `Smoker` (dummy variable): 

>  + log of `FEV`: The coefficient estimate for the explanatory variable `Smoke` is equal to -0.046067 for the transformed data. That means if the subject smokes the dummy variable takes on the value 1 and therefore its forced expiratory volume decreases by  approximately 4.6067% (= $100*\hat{\beta} \%$) on average. Non-smokers receive the dummy variable equal to zero, which then cancels out the coefficient.

>  + original scale of `FEV`: The coefficient estimate for the explanatory variable `Smoke` is equal to -0.087246 for the original data. That means if the subject smokes the dummy variable takes on the value 1 and therefore its forced expiratory volume decreases by 0.087246 liter on average. 


*ii. `Std. Error`* 

*Discuss the statistic based on the `Age` and `Htmc` predictor. Also explain how a 95% confidence interval can be constructed.*

> The Standard Error provides an estimate of the reliability of our coefficient estimates. The smaller the standard error of the estimate (the uncertainty), the higher the statistical significance. 

> Here, the estimator `age` has a standard error of 0.003348. The variable `Htcm` has a standard error of 0.000661. With regard to the corresponding estimates (0.023387 and 0.016849) those standard errors can be interpreted as relatively small. 

> In general, the standard error here tells us that we can be 95% confident that our observed sample mean lays within an interval of plus or minus 1.96 standard error from the population mean. This interval is called the 95% confidence interval. 

> Formally expressed the 95% confidence interval is computed as follows: 
$(estimate - 1.96*SE,\ estimate + 1.96*SE)$

*iii. `Residual standard error`*

> The Residual Standard Error is a measure of the quality of a linear regression fit. It is the average amount that the predicted value of the FEV (response variable) of the regression model will deviate from the true value of FEV. It can also be described as the estimate of the standard deviation of the error term. 

> Formally it is given by: 

> $$\text{Residual Standard Error}=\frac{RSS}{(n-p-1)}$$ with \(RSS=\sum^n_{i=1}{(y_i −\hat{y_i})^2}\).

*iv. `F-statistic`*

> The F-test estimates whether there is a relationship between the predictor and response variables of the model. The standard errors mentioned in *2.2.ii.* take on an important role when it comes to hypothesis testing. In a traditional hypothesis test a null hypothesis \(H_0\) is tested against an alternative hypothesis \(H_1\). In this case an example for the null hypothesis would be that there is no relationship between the response `FEV` and the predictors `Age`, `Htcm`, `Gender` and `Smoke`:

> $$H_0: \beta_1 = \beta_2 = ... = \beta_j= \beta_p = 0$$

> And 

> $$H_1: \text{at least one } \beta \text{ is non-zero.}$$


> In this case of hypothesis test we would compute the F-statistic. Formally the F-Statistic would be calculated as follows: 

> $$F=\frac{(TSS-RSS)/p}{RSS/(n-p-1)}$$

> with `n` being the number of observations, `p` being the number of covariates, \(TSS=\sum^n_{i=1}{(y_i −\bar{y})^2}\) and \(RSS=\sum^n_{i=1}{(y_i −\hat{y_i})^2}\). 


## Task 1.4
*What is the proportion of variability explained by the fitted `lung_model`?*

> The coefficient of determination (R-squared) is a goodness of fit measure for linear regression models that can explain how well a model explains the variation of the actual data giving a value between zero and one. 

> For our model the R-squared is equal to 0.8106. Hence 81.06% of the variance in the dependent variable `log(FEV)` can be explained by the independent variables. But the R-Squared value can be misleading. The statistic increases with every regressor added to the model independent of the statistical significance of the regressor. So that models with a lot of regressors can have unfounded high R-squared values. 

> The adjusted R-squared adjusts for the number of terms in the model and gives a more reliable estimate of the fit. It increases when a useful regressor is added and decreases if the added regressor is not useful.

> The difference between the R-squared and the adjusted R-squared is, that R-squared assumes that every single regressor explains the variation in the response variable. Whereas the adjusted R-squared measures the percentage of variation explained by only the regressors that actually affect the dependent variable.

> The adjusted R-squared for our model is equal to 0.8095. Hence 80.95% of the variance in the dependent variable `log(FEV)` can be explained by the independent variables. The adjusted R-squared is slightly smaller than R-squared suggesting that R-squared overestimated the fit of our model.  

## Task 1.5
*The `summary()` output also reports the two statistics `t value` and `Pr(>|t|)` for each coefficient. Briefly explain the hypothesis test that is underlying the t-statistic.*

> When controlling for a single regressor, the hypothesis test can be used to determine whether there is a relationship between the regressor and the dependent variable. 
In that case our null hypothesis is as follows: 

> $$H_0:\hat{\beta_1}  = 0$$

> And the alternative hypothesis then would be: 

> $$H_1:\hat{\beta_1} \neq 0$$

> We want to make sure that our estimate $\hat{\beta_1}$ is substantially far from zero. We do this by looking at the number of standard deviations that $\hat{\beta_1}$ is away from 0. The instrument we use is the t-statistic. Formally the t-statistic is calculated as follows: 

> $t=\frac{\hat{\beta_1}-0}{SE(\hat{\beta_1})}$, 

> with $\hat{\beta_1}$ being the estimate for $\beta_1$ and $SE(\hat{\beta_1})$ being the standard error of that estimate. 

> Based on the t-statistic a p-value can be calculated. The p-value is the probability of achieving a |t| as large as or larger than the observed absolute t value if the null hypothesis was true, where \(H_0\) is \(\hat{\beta_i}=0\):
$${ \text{p-value}}=Pr_{H0}(|t|>|t^{act}|)$$

> At a 5% significance level the hypothesis is rejected in the case that the actual t-value is bigger than 1.96 or the equivalent p-value is less than 0.05. Here the hypothesis stated above can be rejected for all of the coefficients, because all of the t-values are bigger than 1.96.  

## Task 1.6

*Consider a 14-year-old male. He is 175cm tall and does not smoke. What is your best guess for his `log(FEV)`? Construct a 95% prediction interval for his forced expiratory volume `FEV` (remember to inverse the logarithm). Please comment on whether you find this prediction interval useful.*


```{r}
# creating a data frame with the specific predictor values
predictdataframe <- data.frame( Age = 14, Htcm = 175, Gender = "M", Smoke = "0")
pred_log_fev = predict(lung_model, newdata = predictdataframe)

# prediction for log(FEV)
pred_log_fev

# inverse of log
pred_fev = exp(pred_log_fev)
pred_fev
logfev.ci = predict(lung_model, newdata = predictdataframe, level = 0.95, interval = "prediction")
pred_fev.ci = exp(logfev.ci)
pred_fev.ci
```

> In order to predict `log(FEV)` for these specific parameters, we need to create a new data frame with the values for each variable. Then we can apply the linear model to our data. The predicted logarithmized forced expiratory volume of the subject is equal to 1.315203 liter. 

> In order to construct the confidence interval, we need to inverse the predicted logarithmized forced expiratory volume. Next we can calculate the 95% confidence interval for `FEV` equal to 3.901148:

> The 95% confidence interval is equal to 2.928889 up to 5.196153 liter, which is quite wide and limits the power of predicting. 

## Task 1.7

*Redo the multiple linear regression, but add an interaction term between `Age` and `Smoke` and print the results. What is the meaning of the estimate for the interaction term? Is the interaction term statistically significant? What is the effect of the inclusion of the interaction term on the statistical significance of `Smoke` and `Age`?*

```{r}
# adding the interaction term
lung_model_interaction <- lungcap %>%
lm(log(FEV) ~ Age + Htcm + Gender + Smoke + Age*Smoke, data = .)     
summary(lung_model_interaction)
summary(lung_model)
```

> The estimate for the interaction term between the continuous regressor `Age` and the dummy regressor `Smoke` is equal to -0.0116659. Looking at the `Age` estimator we can see that with each additional year of age the lung volume (`FEV`) increases by 2.5368%. If the person smokes, then the `FEV` increase with age is counteracted by the interaction term. Then the `FEV` increases by: $$\Delta FEV \% = 2.5368\%+(-1.1666\%)=1.3702\%$$ 

# Task 2: Classification (Logit/LDA/k-NN)


```{r, message=F}
tennis <- readr::read_csv(file = "./tennisdata.csv") %>% 
  dplyr::mutate(across(Result, as.factor))                  

library(yardstick)

tennis
```

*The following code chunk computes the quality scores and stores them in a `tibble` together with the result (`y`) of each match (missing values are removed via `tidyr::drop_na()`).*
*The quality scores are calculated for each match and each player:*

$Quality statistic(x_c) = ass - double fault - unforced fault  = ACE_c − UFE_c − DBF_c$   

*with* $c=1, 2$

```{r}

tennis <- tennis %>% 
  dplyr::mutate(
    x1 = ACE.1 - UFE.1 - DBF.1,               
    x2 = ACE.2 - UFE.2 - DBF.2,
    .before = Result
  ) %>%
  dplyr::select(Result, x1, x2) %>% 
  dplyr::rename(y = Result) %>% 
  tidyr::drop_na()                          

tennis

```
*In the new tibble tennis the match result (1 = player 1 wins) is the response variable with the player`s quality score as the explanatory variables.*


*Next, perform a 50:50 train-test split using the `rsample` package. First, split the data set into two equal parts using `initial_split()`. Second, extract the training and test set from the split object using `training()` and `testing()`.*

```{r}
set.seed(2020)

tennis_split <- tennis %>% 
  rsample::initial_split(prop = 0.5)

train_set_tn <- rsample::training(tennis_split)
test_set_tn <- rsample::testing(tennis_split)
```

*In the scatter plot that results from the following `R` code, the training (test) observations are shown as triangles (full circles). Matches won by player one (two) are displayed in dark orange (blue).*

```{r}
ggplot2::ggplot() +
  geom_point(data = train_set_tn, aes(x1, x2, color = y, shape = "Training set")) + 
  geom_point(data = test_set_tn, aes(x1, x2, color = y, shape = "Test set")) + 
  scale_color_manual(values = c("blue4", "darkorange1")) +
  theme_classic() +
  theme(legend.title = element_blank())
```

## Task 2.1

*Get a general overview of the data frame `tennis`. How many observations are there? What are the median values of `x1` and `x2`? Display the matrix of pairwise scatterplots and briefly comment on the relationship between `y` and `x1` respectively `y` and `x2`.*


```{r}
# getting an overview
names(tennis)
summary(tennis)

# number of observations
tennis %>%
  tally()

# and median values
median(tennis$x1)
median(tennis$x2)

# plot matrix of pairwise scatterplots 
pairs(tennis, col=tennis$y)
```

> The higher the quality statistic of the player, the more likely he is to win the game. That means, if `x1` is high, `y` is more likely to be equal to one and if `x2` is high, `y` is more likely to be equal to zero in the data set column (!).


## Task 2.2

*Perform a logistic regression on the full data set and print the results using `summary()`. In addition, address the following questions:*


```{r}
# perform logistic regression
glm_tennis <- glm( formula = y ~ x1 + x2, data = tennis, family = binomial)

# print summary
summary(glm_tennis)
```


*i. Are the estimated coefficients for `x1` and `x2` statistically significant?*                         

> The estimated coefficients for `x1` and `x2` are both statistically significant at a significance level of <0.001. So we can reject the null hypotheses of both estimates being equal to zero. Therefore, it is safe to say that the quality statistics of both players have an effect on the results of the games.    

*ii. Just by looking at the two coefficients: What is the effect on `y` if both `x1` and `x2` increase by one?*

```{r}
summary(glm_tennis)$coefficients[2, 1]+summary(glm_tennis)$coefficients[3, 1]

```

> If both `x1` and `x2` increase by one unit, then y increases by 0.003404365. This means if the quality statistic of both players increases by one unit the probability of player one winning increases by 0.34%. The effect would be minimal.

*iii. What is the effect on the odds of success for player 1 if `x1` increases by one?*


```{r}
summary(glm_tennis)$coefficients[2, 1]
```

> If the quality statistic of player one increases by one unit the odds of success for player one increase by 8.76%.

*iv. In the first match, player 1 has a quality of -25 and player 2’s quality is -20. What is the value for the odds-ratio for the given prediction and how can it be interpreted? What is the predicted probability that player 1 wins this match? Did player 1 actually win the the match?*

```{r}

explanatory_data <- tibble(x1 = -25, x2 = -20)

prediction <- explanatory_data%>%
  mutate(success = predict(glm_tennis, explanatory_data, type = "response"),
         failure = (1-predict(glm_tennis, explanatory_data, type = "response")),
         most_likely = round(success), 
         odds_ratio = success/(1-success))


prediction
```

> If player one has a quality score of -25 and player two has a quality score of -20 the odds-ratio for this prediction is equal to 0.620483. To interpret the value it is important to know the definition of the odds-ratio:

>$$\text{odds ratio} = \frac{ \text{probability of success for player one}}{ 1- \text{probability of success for player one}}$$


> The odds-ratio is the probability of success for player one divided by the probability of failure for player one. The predicted probability of success for player one is 0.3829 (38.29%) and the probability of failure for player one is 0.617 (61.7%). The odds-ratio is \(\frac{ \text{0.3829}}{\text{0.617}}=\text{0.620483}\). This is equivalent to the statement that approximately six successes face ten failures. 

```{r}
predited_probability <- prediction$success
predited_probability

predicted_result <- round(prediction$success)
predicted_result

actual_result <- tennis[1,1]
actual_result

actual_result == predicted_result
```

> The predicted probability by the logistic regression of player one winning the game is equal to 38.29%. That means the model predicts an outcome of \(\text{y}=\text{0}\) which means that player one will lose the game. The actual result of the game was that player one lost. That means the model successfully predicted the actual outcome. 


## Task 2.3

*The receiver operating characteristic (ROC)-curve is a visual tool that enables you to compare the performance of different classifiers. The trajectory of the ROC-curve is derived by systematically varying the probability threshold which determines if a predicted probability is assigned to class 0 (loss player 1) or class 1 (success player 1). Use the predictions of the logistic regression model fitted in the previous task to construct a ROC-curve. In addition, answer the following questions:*


```{r}
# vector of true game results
actual_response <- tennis$y

# vector of predicted game results 
predicted_rounded_response <- round(fitted(glm_tennis))

# vector of non rounded predicted game results 
predicted_responses <- predict(glm_tennis, tennis, type = "response")

# creating a table of counts 
outcomes <- table(predicted_rounded_response, actual_response)

# conferting the table of counts to a yardstick confusion matrix 
confusion_matrix <- conf_mat(outcomes)
confusion_matrix

# plotting the confusion matrix 
autoplot(confusion_matrix)

# performance metrics 
summary(confusion_matrix, event_level = "second")

# ROC curve 
df_roc_curve <- data.frame(actual_response, predicted_responses)
ROC <- roc_curve(df_roc_curve, 
                 truth = tennis$y,
                 predicted_responses,
                 event_level = "second")
autoplot(ROC)
```

*i. What is the model's predictive accuracy (i.e. proportion of correctly predicted data points)?*
```{r}
summary(confusion_matrix)%>%
  slice(1)
```
> The models predictive accuracy is equal to 0.747141. That means the model predicts 74.71% of the game results correctly. 

*ii. What is its area under the receiver operator curve (ROC-AUC)?*

> The area under the ROC curve can be seen as a summary of the model`s accuracy. It measures a model's ability to discriminate the positive from the negative class. The ROC-AUC value lies between 0 and, where 1 represents a perfect model with a 100% accuracy of classification.  

```{r}
colAUC(predicted_responses, actual_response)
```
> The tennis model has an ROC-AUC value of 81.21%, which can be considered as relatively high. It indicates a high accuracy of the models ability to predict the right classification. 

*iii. What is its sensitivity and specificity assuming a probability cutoff of 0.5? How can these two measures be interpreted?*

```{r}
#pull sensitivity 
summary(confusion_matrix)%>%
  slice(3)

#pull specificity 
summary(confusion_matrix)%>%
  slice(4)
```

> Sensitivity measures the proportion of true positives. Here Sensitivity takes on the value 76.79%. That means 76.79% of the true successes of player one are also predicted as such. Specificity is the proportion of observations where the actual response was false where the model also predicted that they were false, which is 72.66% of true failures being accurately predicted. 
  
*iv. Taking all of the above into account, how would you interpret the performance of the model in your own words?*

> All of the performance indicators (Sensitivity, Specificity, Accuracy and ROC-AUC) can be regarded as relatively high. 

## Task 2.4

*Perform a logistic regression with only the training data set and compute the confusion matrix for the test set. What is the accuracy, sensitivity and specificity? How does the model performance compare to the previous task where the model is fitted on the whole data?*


```{r}
# log regression with only the training set
glm_tennis_train <- glm(y ~x1 + x2, family=binomial, data=train_set_tn)

# vector of true game results
actual_response_test <- test_set_tn$y

# vector of predicted game results with the test set data 
predicted_rounded_response_test <- round(predict(glm_tennis_train, test_set_tn, type = "response"))

# creating a table of counts 
outcomes_test <- table(predicted_rounded_response_test, actual_response_test)

# converting the table of counts to a yardstick confusion matrix 
confusion_matrix_test <- conf_mat(outcomes_test)

# plotting the confusion matrix
autoplot(confusion_matrix_test)

# performance metrics of train
summary(confusion_matrix_test, event_level = "second")

# performance metrics of whole data
summary(confusion_matrix, event_level = "second")

```

> The accuracy (74,81%) and specificity (77,07%) of the model are both slightly higher for the model fitted on the training data whereas the sensitivity (72,34%) is slightly lower. More true positive and negative values are predicted correctly. Looking on the true positive and negative values in more detail, we can see that while more true negative values are being predicted correctly, true positive values are being predicted less correctly. 

## Task 2.6

*Perform a linear discriminant analysis (LDA) using `MASS::lda()` with only the training set and compute the confusion matrix for the test set. What is the accuracy, sensitivity and specificity? How does the model performance compare to the logit model in the previous task?*

*Note: In the online lecture you have learned that LDA is preferable if the classes are well-separated, the predictors follow a normal distribution and n is relatively small. Even though these requirements may not be fulfilled here, you may still use the method to compare its performance with the prior results.*

```{r}
# fit the model 
lda_tennis_train <- lda(y~., data = train_set_tn)
lda_tennis_train
# make predictions 
predictions_lda_train <- lda_tennis_train%>%
                     predict(train_set_tn)
# model accuracy 
mean_lda <-mean(predictions_lda_train$class==train_set_tn$y)
mean_lda


```


```{r}

# get the predicted responses from the LDA
predictions_lda_test <- lda_tennis_train%>%
                     predict(test_set_tn)

predictions_lda_rounded <- predictions_lda_test$class

# creating a table of counts 
outcomes_test_lda <- table(predictions_lda_rounded, actual_response_test)

# converting the table of counts to a yardstick confusion matrix 
confusion_matrix_test_lda <- conf_mat(outcomes_test_lda)
confusion_matrix_test_lda

# plotting the confusion matrix 
autoplot(confusion_matrix_test_lda)

# performance metrics of train
summary(confusion_matrix_test_lda, event_level = "second")

# performance of the logit model in the previous task
summary(confusion_matrix_test, event_level = "second")
```

> The accuracy is 75.83%, the sensitivity is 73.4% and the specificity is 78.05% for the LDA model. Comparing the performance statistics with the logistic regression model we can see that the LDA model performance better regarding the accuracy, sensitivity and specificity. The LDA model has an overall better performance than the logistic model. 


## Task 2.7

*Suppose you know about both players’ quality in a specific match in the test set but you do not know the outcome `y`. According to LDA, how many match results (from the test data) can you predict with a probability larger than 80%? Put differently: in how many cases is the LDA more than 80% sure about the match outcome?*

```{r}
# getting the sum of match outcome that either are with prob. more than 80% 'player 1 wins' or less than 20% 'player 1 wins'
sum(predictions_lda_test$posterior[,1]>0.8, predictions_lda_test$posterior[,1]<0.2)



```

> In 97 matches the LDA is more than 80% sure about the outcome (player1 success or player1 failure).


*Use the following `R` code to compute the misclassification error on the train and test set using k-Nearest-Neighbour (k-NN)  for all `k = 1, 2, ..., 30`. The code leverages the `map_dfc()` function from the `purrr` package to apply the `knn()` function from the `class` package to each element in `k` (i.e. the parameters 1 to 30) which yields a column-wise `tibble`. Note that `knn()` requires a data frame object as input to the arguments `train` and `test` and a vector of type `fct` as the input to the `cl` argument.*

*Note: Try to run the following code chunk piece by piece in order to grasp the logic underlying the computations.*


```{r}
# save named integer vector to the variable k
k <-  seq(1, 30, 1) %>% 
  purrr::set_names(1:30)

# apply the knn() function to each element in k
knn_mod_train <- purrr::map_dfc(
  .x = k,
  .f = ~ class::knn(
    train = train_set_tn %>% dplyr::select(x1, x2),
    test = train_set_tn %>% dplyr::select(x1, x2),
    cl = train_set_tn$y,
    k = .x
  )) %>% 
  # check for each prediction (and across all k) if it is unequal to the true class
  dplyr::mutate(across(everything(), ~ (. != train_set_tn$y))) %>% 
  # compute the misclassification error for each k
  dplyr::summarize(across(everything(), mean)) %>%
  # reshape into longer format
  tidyr::pivot_longer(cols = everything(), names_to = "k", values_to = "train_error") %>% 
  # format k as numeric
  dplyr::mutate(across(k, as.numeric))

knn_mod_train

# repeat the same iteration to receive the test set errors
knn_mod_test <- purrr::map_dfc(
  .x = k,
  .f = ~ class::knn(
    train = train_set_tn %>% dplyr::select(x1, x2),
    test = test_set_tn %>% dplyr::select(x1, x2),
    cl = train_set_tn %>% dplyr::pull(y),
    k = .x
  )) %>% 
  dplyr::mutate(across(everything(), ~ (. != test_set_tn$y))) %>% 
  dplyr::summarize(across(everything(), mean)) %>% 
  tidyr::pivot_longer(cols = everything(), names_to = "k", values_to = "test_error") %>% 
  dplyr::mutate(across(k, as.numeric))

knn_mod_test
```

*In a more systematic way, the optimal `k` can be identified via CV (so-called hyperparamter tuning with `k` being the hyperparameter of interest). When conducting hyperparameter tuning the data is usually split into three different sets:*

*- the training set, used for fitting the model,*
*- the validation set, used for finding the optimal hyperparameter, and*
*- the test set, used for computing a robust estimate of the misclassification error on unseen data.*

*Consider the code below where the original training data is further divided into 5 disjunct folds using `vfold_cv()` from the `rsample` package.*

```{r}
set.seed(2020)

train_set_tn_cv <- train_set_tn %>%
  rsample::vfold_cv(v = 5, repeats = 1)

train_set_tn_cv
```

*As you can see, the output of `vfold_cv()` is a `tibble` with an `id` column as well as a list column which contains the cross-validation `splits`. Each split contains 80% of the observations as training data (which can be accessed via `rsample::analysis()`) and 20% as validation data (which can be accessed via `rsample::assessment()`).*

*Next, take the `train_set_tn_cv` data frame and add a new column that stores the hyperparameter values that are supposed to be used in the k-NN model. By using `unnest()`, the five splits are duplicated 30 times (once for each hyperparameter candidate `k`). *

```{r}
train_set_tn_cv <- train_set_tn_cv %>% 
  dplyr::mutate(k = list(k)) %>% 
  tidyr::unnest(cols = "k")

train_set_tn_cv
```

*Now, for each split fit 30 k-NN models, resulting in a total of 150 fitted models. Thus, use `purrr::map2()` to iterate over each of the 150 splits (`.x = splits`) and potential values for `k` (`.y = k`) and gather the predictions of the k-NN models in the `y_pred` column. Subsequently, add the true values for `y` to the `tibble` and store them in the `y_true` column.*

*Note: In a last step the code also discards columns no longer required for further analyses (e.g., the `splits` column). This is especially important in cases where you have to duplicate data which may limit free memory capacities quickly - especially when working with big data.*

```{r}
train_set_tn_cv <- train_set_tn_cv %>% 
  # iterate over all splits and candidates for k and fit a k-NN model
  # store the output (i.e. the predictions) in a column named y_pred
  dplyr::mutate(
    y_pred = purrr::map2(
      .x = splits,
      .y = k,
      .f = ~ class::knn(
        train = rsample::training(.x) %>% dplyr::select(x1, x2),
        test = rsample::testing(.x) %>% dplyr::select(x1, x2),
        cl = rsample::training(.x) %>% dplyr::pull(y),
        k = .y
      ))) %>% 
  # add the true predictions back to the data frame
  dplyr::mutate(
    y_true = purrr::map(
      .x = splits,
      .f = ~ rsample::testing(.x) %>% dplyr::pull(y)
    )) %>% 
  # discard columns
  dplyr::select(-splits)

train_set_tn_cv
```

*Disclaimer: Grasping the inner workings of* `map_*()` *functions is not trivial at the beginning. Ideally, you can comprehend the code above in order to eventually be able to write `purrr`-iterations yourself.*

*Finally, one last computation is required:*

```{r}
knn_mod_cv <- train_set_tn_cv %>% 
  dplyr::mutate(
    quantity = purrr::map2(
      .x = y_pred,
      .y = y_true,
      .f = ~ (.x != .y) %>% mean)
    ) %>% 
  tidyr::unnest(cols = "quantity") %>% 
  dplyr::select(-y_pred, -y_true)

knn_mod_cv
```



### Task 2.8

*Look at the data frame `knn_mod_cv` in the previous code chunk. What is the meaning of the `quantity` column? For each `k`, compute the average CV error as well as its standard error using `dplyr::group_by()` and `dplyr::summarise()`. Which `k` corresponds to the smallest CV error? Which `k` corresponds to the smallest CV error that still satisfies the one-standard error-rule?*

*Hint: The resulting data frame should look similar to `knn_test_errors` and `knn_train_errors` containing the three columns: `k`, `cv_error_mean`, `cv_error_sd`. Otherwise, you may have to adjust variable names employed in the plot in task 2.9.*
```{r}

# preparing dataset (average CV error mean and standard error for each k)
knn_mod_cv <- knn_mod_cv %>%
  dplyr::group_by(k) %>%
  dplyr::summarise(mean(quantity), sd(quantity))

knn_mod_cv
names(knn_mod_cv) <- c("k", "cv_error_mean", "cv_error_sd")

knn_mod_cv

# k corresponding to smallest CV error mean
knn_mod_cv %>%
  arrange(cv_error_mean) %>%
  slice_head(n = 1)

# model with smallest possible k satisfying the one standard error rule 
knn_mod_cv[29,] %>%
  mutate(onestdrule = cv_error_mean + cv_error_sd)

knn_mod_cv %>%
  subset(cv_error_mean <= 0.3358001 & cv_error_mean >= 0.2944174 ) %>%
  slice_head(n = 1)
  

```
> Quantity can be interpreted as the ratio of predicted outcomes and true outcomes for each `k` in all folds of the data. For `k` being 29, we get the smallest CV error mean. Looking for the most parsimonious model, `k` being 5 still satisfies the one-standard error-rule.


### Task 2.9

*Plot the misclassification errors using the code below. What can you say about the bias and variance of the predictions when `k` increases? Why does the CV misclassification error curve consistently exceed the test error curve?*

```{r}
ggplot2::ggplot() +
  # plot train errors
  geom_point(aes(x = k, y = train_error, color = "train_error"), knn_mod_train) +
  geom_line(aes(x = k, y = train_error, color = "train_error"), knn_mod_train) +
  # plot test errors
  geom_point(aes(x = k, y = test_error, color = "test_error"), knn_mod_test) +
  geom_line(aes(x = k, y = test_error, color = "test_error"), knn_mod_test) +
  # plot cv errors
  geom_point(aes(x = k, y = cv_error_mean, color = "cv_error_mean"), knn_mod_cv) +
  geom_line(aes(x = k, y = cv_error_mean, color = "cv_error_mean"), knn_mod_cv) +
  # plot cv error uncertainty
  geom_errorbar(
    aes(
      x = k, y = cv_error_mean,
      ymin = cv_error_mean - cv_error_sd, ymax = cv_error_mean + cv_error_sd
    ),
    knn_mod_cv, width = .5) +
  labs(x = "Number of neighbors", y = "Misclassification error", color = "Legend") +
  theme_classic()
```

> Generally speaking we call this observation the bias-variance trade-off: On the one hand, if the value of `k` is too low, it will give a very flexible classifier that goes along with high variance and low bias and that fits the training set too well (overfitting). The predictions on test data set then won't be especially good. On the other hand, if the value of `k` is too high, the opposite will be true: The classifier will have low flexibility with low variance and high bias. The classifier will consider the general distribution of the observations and not only the nearest points. This makes the predictions more resilient to outliers. As can be seen in the plot with a higher number of nearest neighbors the misclassification error will increase.

> The KNN-method gives a noisy estimate of the test error for each possible model. The model with the best estimate will be selected. By selecting the model with the smallest estimated error, the true error is often bigger than the selected one. This leads to an underestimated test error and hence the test error curve lies below the CV misclassification error curve.  

### Task 2.10

*Run the code below and briefly explain the graph that it creates.*

```{r}
data_grid <- tidyr::expand_grid(
  x1 = seq(min(test_set_tn$x1), max(test_set_tn$x1), length = 100),
  x2 = seq(min(test_set_tn$x2), max(test_set_tn$x2), length = 100))

y_hat_knn <- class::knn(
  train = train_set_tn %>% dplyr::select(x1, x2),
  test = data_grid,
  cl = train_set_tn %>% dplyr::pull(y),
  k = 30)

data_grid %>% 
  dplyr::mutate(y_hat = y_hat_knn) %>% 
  ggplot2::ggplot(aes(x = x1, y = x2, z = as.integer(y_hat))) + 
    geom_point(aes(color = y_hat), shape = ".") +
    geom_point(aes(x = x1, y = x2, z = NULL, color = y), data = test_set_tn) +
    geom_contour(colour = "black", size = .5, bins = 1, lty = "dashed") +
    scale_color_manual(values = c("blue4", "darkorange1")) +
    labs(title = "Nearest Neighbour Classification (k = 30)") +
    theme_classic() +
    theme(legend.title = element_blank())
```

> In the graph we can see a two-class example for KNN classification with the classes `0` and `1`. The colored dots show the true classes, whereas the line represent the decision boundaries for the class prediction. The points of the test data that lie above the decision line will be classified as `0`, which means that Player 2 is predicted to win and the points that lie below the decision line will be classified as `1`, which means that Player 1 is predicted to win. 


### Task 2.11 

*Run the code again, but choose `k = 1`, `k = 50` and `k = 300` in the first line. Compare the graphs. Which of these three models is the most flexible? Explain in one sentence what happens if you would set `k = 500`.*

```{r}
data_grid <- tidyr::expand_grid(
  x1 = seq(min(test_set_tn$x1), max(test_set_tn$x1), length = 100),
  x2 = seq(min(test_set_tn$x2), max(test_set_tn$x2), length = 100))

y_hat_knn <- class::knn(
  train = train_set_tn %>% dplyr::select(x1, x2),
  test = data_grid,
  cl = train_set_tn %>% dplyr::pull(y),
  k = 1)

data_grid %>% 
  dplyr::mutate(y_hat = y_hat_knn) %>% 
  ggplot2::ggplot(aes(x = x1, y = x2, z = as.integer(y_hat))) + 
    geom_point(aes(color = y_hat), shape = ".") +
    geom_point(aes(x = x1, y = x2, z = NULL, color = y), data = test_set_tn) +
    geom_contour(colour = "black", size = .5, bins = 1, lty = "dashed") +
    scale_color_manual(values = c("blue4", "darkorange1")) +
    labs(title = "Nearest Neighbour Classification (k = 1)") +
    theme_classic() +
    theme(legend.title = element_blank())
```

```{r}
data_grid <- tidyr::expand_grid(
  x1 = seq(min(test_set_tn$x1), max(test_set_tn$x1), length = 100),
  x2 = seq(min(test_set_tn$x2), max(test_set_tn$x2), length = 100))

y_hat_knn <- class::knn(
  train = train_set_tn %>% dplyr::select(x1, x2),
  test = data_grid,
  cl = train_set_tn %>% dplyr::pull(y),
  k = 50)

data_grid %>% 
  dplyr::mutate(y_hat = y_hat_knn) %>% 
  ggplot2::ggplot(aes(x = x1, y = x2, z = as.integer(y_hat))) + 
    geom_point(aes(color = y_hat), shape = ".") +
    geom_point(aes(x = x1, y = x2, z = NULL, color = y), data = test_set_tn) +
    geom_contour(colour = "black", size = .5, bins = 1, lty = "dashed") +
    scale_color_manual(values = c("blue4", "darkorange1")) +
    labs(title = "Nearest Neighbour Classification (k = 50)") +
    theme_classic() +
    theme(legend.title = element_blank())
```

```{r}
data_grid <- tidyr::expand_grid(
  x1 = seq(min(test_set_tn$x1), max(test_set_tn$x1), length = 100),
  x2 = seq(min(test_set_tn$x2), max(test_set_tn$x2), length = 100))

y_hat_knn <- class::knn(
  train = train_set_tn %>% dplyr::select(x1, x2),
  test = data_grid,
  cl = train_set_tn %>% dplyr::pull(y),
  k = 300)

data_grid %>% 
  dplyr::mutate(y_hat = y_hat_knn) %>% 
  ggplot2::ggplot(aes(x = x1, y = x2, z = as.integer(y_hat))) + 
    geom_point(aes(color = y_hat), shape = ".") +
    geom_point(aes(x = x1, y = x2, z = NULL, color = y), data = test_set_tn) +
    geom_contour(colour = "black", size = .5, bins = 1, lty = "dashed") +
    scale_color_manual(values = c("blue4", "darkorange1")) +
    labs(title = "Nearest Neighbour Classification (k = 300)") +
    theme_classic() +
    theme(legend.title = element_blank())

```


> When it comes to finding the optimal number of `k` in the KNN approach, a visualization of different numbers of neighbors `k` can help. As can be seen in our computations with the different `k` values, with an increasing number of `k` the decision boundary becomes a relatively straight line and therefore gets less noisy. At the same time decision boundary gets less flexible. It is not useful to increase the number of `k` to 500, because the number of nearest neighbour would exceed the actual number of 394 data points in the training set. In a model in which we would use the maximum number of nearest neighbours (393), the classification boundary would always predict a win for Player 1.



# Task 3: Cross-Validation

> In this exercise, you will work with the `Carseats` data set from the `ISLR` package. First, you will try to predict the unit sales at each location using multiple linear regression, and estimate the test error of this regression model using the validation set approach.

```{r}
data(Carseats, package = "ISLR")

Carseats %>% 
  tibble::as_tibble()
```

## Task 3.1

*Fit a multiple linear regression model, called `lin_reg_mod`, that uses `Price`, `Urban`, and `US` to predict `Sales`. Print the results using `summary()`.*

```{r}
# fitting a multiple linear regression with `Price`, `Urban`, and `US` to predict `Sales`
lin_reg_mod <- lm(Sales ~ Price + Urban + US, data=Carseats)
summary(lin_reg_mod)
```

## Task 3.2

*Estimate the test error of this model using the validation set approach. In order to do this, perform the following steps:*

*i. Split the data set into a training set and a validation set, each encompassing half of the data. Use `set.seed(2020)`.* \break
*Hint: You may use the `initial_split()` function from the `rsample` package.*

```{r}
# splitting the data set
set.seed(2020)

Carseats_split <- Carseats %>% 
  rsample::initial_split(prop = 0.5)

train_set_cs <- rsample::training(Carseats_split)
validation_set_cs <- rsample::testing(Carseats_split)
```

*ii. Fit a multiple linear regression model `lin_reg_mod_train` using only the training observations. Briefly compare the results with `lin_reg_mod` (regarding the estimates, standard errors and p-values).*

```{r}
# fitting linear regression model using training observations
lin_reg_mod_train <- lm(Sales ~ Price + Urban + US, data = train_set_cs)

summary(lin_reg_mod_train)

```
> The estimated effect of a one unit increase within the initial multiple linear regression model in Price on Sales units is -0.047574 and thus less negative than the estimated coefficient of -0.054459 in the fitted model on the training observations. Moreover, the standard error is close to each other and the p-value suggests for both models significant estimates to a <0,1% level.

> The estimated effect of the dummy Urban being included on Sales units within the initial multiple linear regression model is   0.032804 and thus suggesting to influence the Sales units in a different direction than the estimated coefficient with a negative estimate of -0.021916 of in the fitted model on the training observations. Importantly, while the standard error is nearly the same, the p-value suggests for both models no significant of the estimates.

> The estimated effect of the dummy US being included on Sales units within the initial multiple linear regression model is 1.242006 and thus higher than the estimated coefficient of 1.200573 in the fitted model on the training observations. Strikingly, the standard error in the fitted model decreased and so the p-value suggests, that the fitted model is significant to a higher level.

*iii. Predict the response for the 200 test set observations and calculate the mean squared error (MSE).*

```{r}

# general function to calculate the MSE 
calc_mse = function(actual, predicted) {
  mean((actual - predicted) ^ 2)
}

# general function to calculate the RMSE 
calc_rmse = function(actual, predicted) {
  sqrt(mean((actual - predicted) ^ 2))
}

# predicting on the testing data set and calculating MSE 
MSE2020 <-calc_mse(actual = validation_set_cs$Sales, predicted = predict(lin_reg_mod_train, validation_set_cs))
MSE2020
```

> The estimated test error of the model is equal to 6.226233.  

*iv. How does your answer to iii. change if you use the random seeds 2018 or 2019 instead of 2020 to split the data set?*

```{r}
# for random data with the starting point 2019
set.seed(2019)

# splitting the data into sets 
Carseats_split <- Carseats %>% 
  rsample::initial_split(prop = 0.5)
train_set_cs_19 <- rsample::training(Carseats_split)
validation_set_cs_19 <- rsample::testing(Carseats_split)

# fit the regression 
lin_reg_mod_train_19 <- lm(Sales ~ Price + Urban + US, data = train_set_cs_19)


# predict on the testing data set and calculate MSE 
MSE2019 <-calc_mse(actual = validation_set_cs_19$Sales, predicted = predict(lin_reg_mod_train_19, validation_set_cs_19))
MSE2019

# for random data with the starting point 2018
set.seed(2018)

# splitting the data into sets 
Carseats_split <- Carseats %>% 
  rsample::initial_split(prop = 0.5)
train_set_cs_18 <- rsample::training(Carseats_split)
validation_set_cs_18 <- rsample::testing(Carseats_split)

# fit the regression 
lin_reg_mod_train_18 <- lm(Sales ~ Price + Urban + US, data = train_set_cs_19)


# predict on the testing data set and calculate MSE 
MSE2018 <-calc_mse(actual = validation_set_cs_18$Sales, predicted = predict(lin_reg_mod_train_18, validation_set_cs_18))
MSE2018

# MSE for seed 2020
MSE2020

```

> The `MSE2018` is 6,321065 and `MSE2019` is 6,40898. For `MSE2020`, it is 6,226233. The estimated test error differs by up to 0.182747 units. This is only natural seeing that our models are based on different sets of observations. Still the difference is relatively small suggesting that our model predicts quite accurately even based on unseen data. 

*v. Compute the LOOCV estimate for the MSE using the `cv.glm()` function from the `boot` package.* \break
*Hint: The mean squared error (MSE) can be extracted from the resulting list via `$delta`.*

```{r}

# fitting the regression to the whole data set 
lin_reg_model_glm <- glm(Sales ~ Price + Urban + US, data = Carseats)

# calculating and extracting the MSE estimate based on leave one out cross validation 
MSE_LOOCV <-cv.glm(Carseats, lin_reg_model_glm)$delta[1]
MSE_LOOCV

```

> The estimated test error based on the LOOCV method is equal to 6.168591.The error is slightly smaller than the estimated error based on the validation approach. This is typical for both methods because having a bigger data set will normally reduce the test error. 

## Task 3.3

*Use the `regsubsets()` function from the `leaps` package to find the best subset consisting of three predictors to estimate `Sales`, excluding `ShelveLoc`. Apply the function in a way to conduct stepwise forward selection. What are the three predictors selected by this approach? Fit a multiple linear regression using these three predictors and compute the LOOCV estimate for the MSE. Compare with your answer to Task 3.2 v. Can you explain the difference?*

```{r}

# stepwise forward selection function with max three predictors 
lm_sfs <- regsubsets(Sales ~ CompPrice + Income + Advertising + Population + Price + Age + Education + Urban + US, data = Carseats, nvmax = 3, method = "forward")

# summary of the three possible models 
summary(lm_sfs)

# fitting the model using the three best regressors 
lm_sfs_best <- glm(Sales ~ Price + CompPrice + Advertising, data = Carseats)

# calculating and extracting the MSE estimate based on leave one out cross validation 
MSE_LOOCV_3 <-cv.glm(Carseats, lm_sfs_best)$delta[1]
MSE_LOOCV_3

# compare to MSE when fitting all regressors 
MSE_LOOCV

```


> The best subset consists of the three predictors `Price`, `CompPrice` and `Advertising` to estimate `Sales`.

> For this model the estimate test error is equal to 4.411163 and therefore smaller than the test error for the complete model (6.168591). The complete model has a higher complexity (more parameters to predict) and therefore a higher variance that leads to a higher test error. The simpler model with only three predictors leads to predictions that are more accurate. 

## Task 3.4

*Compute the mean of the response (`Sales`). In a next step, you want to understand the potential distribution of the mean. For this purpose, create 20 bootstrapped replicates of the data, using random seed 2020 (`set.seed(2020)`). Then apply the mean function to each bootstrapped replicate. Enter the command 20 times and write down the range of the mean, i.e. the lowest and the largest number.*


```{r}
# getting the mean of the response variable Sales units 
mean_sales <- mean(Carseats$Sales)
mean_sales

# random data set 
set.seed(2020)

# bootstraping with 20 replicates 
resample <- bootstraps(Carseats, times = 20) %>%
{map_dbl(.$splits, 
        function(x){
                     dat <- as.data.frame(x)$Sales
                     mean(dat)
        })}

# sorting the estimated means in a descending order 
sort(resample, decreasing = FALSE)

```
> The range of the mean of the response `Sales` over all 20 bootstrap replicates is 7.114875 to 7.857625.

## Task 3.5

*Repeat the above analysis using `boot()` and `set.seed(2020)`. What is the 99% confidence interval for the mean?*

```{r}
#get the mean of the response variable Sales units 
mean_sales <- mean(Carseats$Sales)
mean_sales

#random data set 
set.seed(2020)

#define a function for the mean of sales 
mean_boot <- function(data, indices){
  dt<-data[indices,]
  c(
    mean(dt[,1]) )}
   
#bootstraping with 20 replicates 
mean_sales_boots <- boot(data = Carseats, statistic = mean_boot,
   R = 20)

#creating a data frame with all means calculated by the bootstraps 
mean_sales_boots_data <- data.frame(mean_sales_boots$t)

#creating the 99% confidence intervall 
CI(mean_sales_boots_data$mean_sales_boots.t, ci=0.99)

```
> The 99% confidence interval for the distribution of the mean of sales units estimated by bootstraps is equal to `[7.390621, 7.591301]`.


# Task 4: Linear Model Selection and Regularization

*In this final exercise, you will predict diamond prices using the `diamonds` data set from the `ggplot2` package. The diamonds data set consists of:*

*- `price` (in US dollars) - which will be the response,*

*and quality information (9 predictors) for around 54,000 diamonds. There are four C’s of diamond quality:*

*- `carat` (weight),*
*- `cut` (quality of the cut: Fair/Good/Very Good/Premium/Ideal),*
*- `colour` (from worst J to best D) and*
*- `clarity` (from worst to best: I1, SI1, SI2, VS1, VVS1, VVS2, IF).*

*In addition, there are five physical measurements:*

*- `depth` (total depth percentage, calculated from x, y and z),*
*- `table` (width of top of diamond relative to widest point),*
*- `x` (length in mm),*
*- `y` (width in mm), and*
*- `z` (depth in mm).*

```{r}
data(diamonds, package = "ggplot2")

diamonds
```

*To make things easier (in terms of runtime), the following code chunk reduces the data to a tenth of its size.*

```{r}
set.seed(2020)

diamonds <- diamonds %>% 
  slice_sample(n = nrow(.) / 10)

diamonds 
```

*Your aim will be to predict `price`, based on some or all of the predictors. Ideally, you want to understand which of the predictors are important in estimating the price and how the predictors are related to the price.*


## Task 4.1

*Get an overview of the `diamonds` data. What are the three highest prices in the data set? How many carats do those diamonds weigh? What is the mean weight? Which color is the most prevalent? Plot `price` against `carat` as well as their logged forms against each other using `ggplot()`.*

```{r}
# highest prices in the data set
diamonds[,c(1,7)] %>%
  arrange(desc(price)) %>%
  slice_head(n=3)

# mean weight
diamonds %>%
  summarise(meanWeight = mean(carat))

# most prevalent color
diamonds %>%
  count("color") %>%
  arrange(desc(freq)) %>%
  slice_head()

# plot price against carat
ggplot(diamonds,aes(x=carat,y=price))+
  geom_point(color='darkblue',fill='blue')+
   ggtitle('Diamond price vs. carat')

# plot log forms of price and carat
ggplot(diamonds, aes(x = log(carat), y = log(price))) +
    geom_point(color = "darkgreen")+
  ggtitle('Log: Diamond price vs. carat')

```

> The highest prices in the data set are 18787\$, 18757\$ and 18735\$. The mean weight of all diamonds in the data set is 0.7923304 carat. The most prevalent color is G.

## Task 4.2

*Perform forward and backward stepwise selection to choose the best subset of predictors for `log_price`. Compare the two results qualitatively as well as visually (by plotting the fitted objects using `plot()`). Using adjusted R² as decision criterion, how large is the best subset from the backward stepwise selection?*

*Hint: Adjusted R² values can be extracted from the fitted model using `summary(model)$adjr2`.*

```{r}
# creating the log_price regressor 
diamonds_log_price <- diamonds %>%
  mutate(log_price = log(price))

# we choose the method with ordered factors here
# forward stepwise selection
lm_diamonds_fss <- regsubsets(log_price ~ carat + cut + color + clarity + x + y + z+ depth + table, data = diamonds_log_price, method = "forward")

# coefficients of the best subset of predictors using the forward stepwise selection 
coef(lm_diamonds_fss, which.min(summary(lm_diamonds_fss)$cp))

# backward stepwise selection 
lm_diamonds_bss <- regsubsets(log_price ~ carat + cut + color + clarity + x + y + z+ depth + table, data = diamonds_log_price, method = "backward")

# coefficients of the best subset of predictors using the backward stepwise selection 
coef(lm_diamonds_bss, which.min(summary(lm_diamonds_bss)$cp))

# plotting the results 
plot(lm_diamonds_fss, scale = "Cp", xlab = "possible predictors")
plot(lm_diamonds_bss, scale = "Cp", xlab = "possible predictors") 

# adjusted R-squared as the decision criterion; the adjusted R-squared values of all possible subsets 
bss_summary = summary(lm_diamonds_bss)$adjr2


# plotting the possible models against their adjusted R-squared value in order to select the best subset 
plot(lm_diamonds_bss, scale = "adjr2", xlab = "possible predictors")

# the corresponding adjusted R-squared value 
max(bss_summary)

```
> Computing the forward and backward selection approach, gives us 8 best predictors as printer above. The plots show the possible coefficients and their corresponding Mallow's `Cp`, if added, which should be minimized. Both methods suggest the same predictors as the best subset of all available predictors.

> If we use the adjusted R-squared value as our decision criterion the best subset would consist of 8 predictors as it can be seen in the plot. The adjusted R-squared for the best subset is equal to 0.9781214. That means the best subset can explain 97.81% of the variance in the response variable `log_price`.  


## Task 4.3

*What are the main differences between using adjusted R² for model selection and using cross-validation (with mean squared test error, i.e. MSE)?*

> The adjusted R-squared is a commonly known tool when it comes to evaluating how well a model fits the actual data. 

> Formally expressed the adjusted R-squared for a least squares model with d variables is the following: 
$$\text{Adjusted R}^2=1-\frac{RSS/(n-d-1)}{TSS/(n-1)}$$

> A high adjusted R-squared means that the test error is small. This is different for alternative approaches such as `Cp`, `AIC` and `BIC`, where a small error would be indicated by a small value. 

> The cross-validation-approach calculates an error for each model and selects the `k` with the smallest estimated test error. One advantage of this approach towards the adjusted R-squared is that an estimate of the error variance here is not required. Furthermore it makes few assumptions about the true model and can be used in complex cases where the degrees of freedom cannot be identified. On the other hand, the adjusted R-squared is commonly known among most scientists and is easy to interpret. 



## Task 4.4

*Use Lasso and Ridge regression on the data to predict the `log_price`. Therefore, employ the `cv.glmnet()` function from the `glmnet` package and fit the models using the MSE as loss function and `nfolds = 5` as the number of folds. What is the optimal hyperparameter lambda and the corresponding MSE in both cases? Which predictors are selected by the optimal Lasso model?*

*Hint:  Unfortunately, you can not feed a data frame object to the first argument `x` of `cv.glmnet()`. Instead, the function requires a predictor matrix as input. To convert the `diamonds` data frame into a matrix object you may use the `model.matrix` function.*
 

```{r}
# log-transformation to price variable
diamonds_LassoRidge <- diamonds %>%
  mutate(price = log(price))

x_vars <- model.matrix(price ~ . , diamonds_LassoRidge)[,-1]
y_var <- diamonds_LassoRidge$price
lambda_seq <- 10^seq(2, -2, by = -.1)

# splitting the data into test and training set
set.seed(2020)
train=sample(1:nrow(x_vars), nrow(x_vars)/2)
x_test = (-train)
y_test = y_var[x_test]

# the lasso model 
cv_output <- cv.glmnet(x_vars[train,], y_var[train], alpha = 1, lambda = lambda_seq, nfolds = 5, type.measure = "mse")
cv_output

# plotting the lasso model 
plot(cv_output)

# identifying the best lambda
best_lam <- cv_output$lambda.1se
best_lam

# fit the model 
fit_lasso_se <- glmnet(x_vars[train,], y_var[train], alpha = 1, lambda = cv_output$lambda.1se)

# list of nonzero coefficients 
fit_lasso_se$beta[,1]

# list of all coefficients 
coef(fit_lasso_se)

# ridge model 
cv_output_ridge <- cv.glmnet(x_vars[train,], y_var[train], alpha = 0, lambda = lambda_seq, nfolds = 5, type.measure = "mse")
cv_output_ridge

# plotting the ridge model 
plot(cv_output_ridge)

# identifying the best lambda 
best_lam_ridge <- cv_output_ridge$lambda.1se
best_lam_ridge

# fit the model
fit_lasso_ridge <- glmnet(x_vars[train,], y_var[train], alpha = 1, lambda = cv_output_ridge$lambda.1se)

# list of nonzero coefficients 
fit_lasso_ridge$beta[,1]

# list of all coefficients 
coef(fit_lasso_ridge)
```
> The optimal hyperparameter lambda for the Lasso Regression is 0.01258925 with a corresponding `MSE` of 0.03395
> The optimal hyperparameter lambda for the Ridge Regression is 0.01995262 with a corresponding `MSE` of 0.02516.
> In the opimal Lasso model the following predictors are selected: `color.L`, `color.Q`, `clarity.L`, `clarity.Q`, `depth`, `x`, `y` and `z`.






## Sources
*Some of the exercises are based on those from other machine/statistical learning courses, i.e. by the Norwegian University of Science and Technology (NTNU) and the Albert-Ludwigs-Universität Freiburg.*
